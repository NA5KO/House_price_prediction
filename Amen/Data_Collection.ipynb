{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6f65fc-5389-4956-8cc5-e93779ba3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc308c1d-fc00-47eb-84dd-b1f0393f0a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.tunisie-annonce.com/AnnoncesImmobilier.asp\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86264eb-0555-4389-8781-b65ade21885b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully.\n",
      "Page 2 scraped successfully.\n",
      "Page 3 scraped successfully.\n",
      "Page 4 scraped successfully.\n",
      "Page 5 scraped successfully.\n",
      "Page 6 scraped successfully.\n",
      "Page 7 scraped successfully.\n",
      "Page 8 scraped successfully.\n",
      "Page 9 scraped successfully.\n",
      "Page 10 scraped successfully.\n",
      "Page 11 scraped successfully.\n",
      "Page 12 scraped successfully.\n",
      "Page 13 scraped successfully.\n",
      "Page 14 scraped successfully.\n",
      "Page 15 scraped successfully.\n",
      "Page 16 scraped successfully.\n",
      "Page 17 scraped successfully.\n",
      "Page 18 scraped successfully.\n",
      "Page 19 scraped successfully.\n",
      "Page 20 scraped successfully.\n",
      "Page 21 scraped successfully.\n",
      "Page 22 scraped successfully.\n",
      "Page 23 scraped successfully.\n",
      "Page 24 scraped successfully.\n",
      "Page 25 scraped successfully.\n",
      "Page 26 scraped successfully.\n",
      "Page 27 scraped successfully.\n",
      "Page 28 scraped successfully.\n",
      "Page 29 scraped successfully.\n",
      "Page 30 scraped successfully.\n",
      "Page 31 scraped successfully.\n",
      "Page 32 scraped successfully.\n",
      "Page 33 scraped successfully.\n",
      "Page 34 scraped successfully.\n",
      "Page 35 scraped successfully.\n",
      "Page 36 scraped successfully.\n",
      "Page 37 scraped successfully.\n",
      "Page 38 scraped successfully.\n",
      "Page 39 scraped successfully.\n",
      "Page 40 scraped successfully.\n",
      "Page 41 scraped successfully.\n",
      "Page 42 scraped successfully.\n",
      "Page 43 scraped successfully.\n",
      "Page 44 scraped successfully.\n",
      "Page 45 scraped successfully.\n",
      "Page 46 scraped successfully.\n",
      "Page 47 scraped successfully.\n",
      "Page 48 scraped successfully.\n",
      "Page 49 scraped successfully.\n",
      "Page 50 scraped successfully.\n",
      "Data extracted and saved to 'real_estate_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\"\"\"print(soup)\"\"\"\n",
    "# Base URL (Replace with the actual URL)\n",
    "base_url = \"http://www.tunisie-annonce.com/AnnoncesImmobilier.asp?rech_page_num=\"  \n",
    "\n",
    "# Data storage\n",
    "data = []\n",
    "headers = [\"Région\", \"Nature\", \"Type\", \"Texte annonce\", \"Prix\", \"Modifiée\"]\n",
    "\n",
    "# Loop through the first 50 pages\n",
    "for page in range(1, 51):  # Adjust range if needed\n",
    "    url = f\"{base_url}{page}\"  # Construct the URL for each page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page}. Skipping...\")\n",
    "        continue  # Skip to the next page if there's an error\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all rows with class 'Tableau1'\n",
    "    rows = soup.find_all('tr', class_='Tableau1')\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 12:  # Ensure there are enough columns\n",
    "            region = cells[1].get_text(strip=True)\n",
    "            nature = cells[3].get_text(strip=True)\n",
    "            type_ = cells[5].get_text(strip=True)\n",
    "            texte_annonce = cells[7].get_text(strip=True)\n",
    "            prix = cells[9].get_text(strip=True)\n",
    "            modifiee = cells[11].get_text(strip=True)\n",
    "\n",
    "            # Append row to data\n",
    "            data.append([region, nature, type_, texte_annonce, prix, modifiee])\n",
    "\n",
    "    print(f\"Page {page} scraped successfully.\")\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"real_estate_data_version_2.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)  # Write headers\n",
    "    writer.writerows(data)  # Write data rows\n",
    "\n",
    "print(\"Data extracted and saved to 'real_estate_data_version_2.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb775d4-a359-427a-95d0-46d2e9ca4288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
